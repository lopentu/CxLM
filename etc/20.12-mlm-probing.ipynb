{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947f7331-799e-457e-9b05-f707abd298e9",
   "metadata": {},
   "source": [
    "## MLM-tuned evals\n",
    "* Inputs:\n",
    "  * raw data: `../data/raw_cx_data.json` (10.01)\n",
    "  * CV splits: `../data/cv_splits_10.json` (10.01)\n",
    "  * MLM-tuned: `../data/models/apricot_mlm_01` (20.10)\n",
    "  * MLM-tuned: `../data/models/apricot_mlm_02` (20.10)\n",
    "* Outputs:\n",
    "  * (none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a002c61d-99e2-4e20-a1fd-8d739a3afd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b8acef-ef12-40d0-9d27-fde1bd8b7f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from hashlib import sha256\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertTokenizerFast, BertForMaskedLM, BertModel\n",
    "from import_conart import conart\n",
    "from conart.mlm_masks import batched_text, batched_text_gan\n",
    "from conart.gan_utils import generate_adversarials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b571992-7a21-4000-80e1-bab0f9d8babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") \\\n",
    "         if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a62670-63f0-45f2-98b1-f8aa45b47866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../data/raw_cx_data.json\"\n",
    "with open(data_path, \"r\", encoding=\"UTF-8\") as fin:\n",
    "    data = json.load(fin)\n",
    "## Check data is the same\n",
    "h = sha256()\n",
    "h.update(pickle.dumps(data))\n",
    "data_hash = h.digest().hex()[:6]\n",
    "assert data_hash == \"4063b4\"\n",
    "len(data) # should be 11642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7556dcc-1a31-40c8-b142-9a41f7e4ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read cv splits\n",
    "with open(\"../data/cv_splits_10.json\", \"r\") as fin:\n",
    "    cv_splits = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46be44f6-5e00-4f5c-ac66-aec6f4441879",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "# model = BertForMaskedLM.from_pretrained('bert-base-chinese')\n",
    "# model = BertForMaskedLM.from_pretrained('ckiplab/bert-base-chinese')\n",
    "model = BertForMaskedLM.from_pretrained('../data/models/apricot_mlm_03')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce38a19-96c3-44c8-9985-a513758804be",
   "metadata": {},
   "source": [
    "## Checking input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d978cb6d-17eb-45b4-95a9-ece138f1f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, test_idxs = cv_splits[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1179ed-8332-46d9-99af-e5ca98da2f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'board': 'BabyMother',\n",
       " 'text': ['再', '擠', '也', '擠', '不', '出來', '了'],\n",
       " 'cnstr': ['O', 'BX', 'IX', 'IX', 'IX', 'IX', 'O'],\n",
       " 'slot': ['O', 'BV', 'BC', 'BV', 'BC', 'BV', 'O'],\n",
       " 'cnstr_form': ['v', '也', 'v', '不', 'X'],\n",
       " 'cnstr_example': ['擠', '也', '擠', '不', '出來']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = data[1211]\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eae2503-404a-4c48-b5f1-432570bc35e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['masked', 'text', 'mindex', 'mindex_bool']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = batched_text(data, train_idxs[:5], \"vslot\")\n",
    "list(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074d156f-9ef2-4692-8e13-0bec68a2911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[135, 137,  -1,  -1,  -1,  -1],\n",
       "       [ 13,  15,  -1,  -1,  -1,  -1],\n",
       "       [  3,   5,  -1,  -1,  -1,  -1],\n",
       "       [  0,   1,   2,   4,   5,   6],\n",
       "       [  0,   2,  -1,  -1,  -1,  -1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"mindex\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a826fa-19ed-488b-b4dd-bae771ec8094",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "914cb830-5686-46a6-8abc-b4d4f75e400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "def make_masked_ylabels(mindex: np.ndarray, ori_ids: torch.Tensor):            \n",
    "    ylabels = torch.zeros_like(ori_ids)-100\n",
    "    for i in range(mindex.shape[0]):\n",
    "        # add 1 for the extra [CLS] token added after tokenizer\n",
    "        j = mindex[i]+1        \n",
    "        j = j[(j>=0) & (j<512-2)]        \n",
    "        ylabels[i, j] = ori_ids[i, j]\n",
    "    return ylabels\n",
    "\n",
    "def compute_loss(logits, ylabels):\n",
    "    return loss_fct(logits, ylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee23edca-115d-479a-826b-7ec5d268e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mlm_loss(model, train_idxs, use_mask, debug=False):\n",
    "    batch_size = 16\n",
    "    loss_vec = []    \n",
    "    \n",
    "    for i in range(0, len(train_idxs), batch_size):\n",
    "        data_idxs = train_idxs[i:i+batch_size]\n",
    "        batch = batched_text(data, data_idxs, use_mask)\n",
    "\n",
    "        # compute perplexity    \n",
    "        X = tokenizer(batch[\"masked\"], return_tensors=\"pt\", is_split_into_words=True, padding=True, truncation=True)\n",
    "        X = X.to(device)\n",
    "        Xori = tokenizer(batch[\"text\"], return_tensors=\"pt\", is_split_into_words=True, padding=True, truncation=True)\n",
    "        ylabels = make_masked_ylabels(batch[\"mindex\"], Xori[\"input_ids\"])\n",
    "        ylabels = ylabels.to(device)        \n",
    "        with torch.no_grad():\n",
    "            out = model(**X)\n",
    "            batch_loss = compute_loss(out.logits.swapdims(2, 1), ylabels)\n",
    "            # average over non-zero token losses\n",
    "            batch_loss = batch_loss.sum(1) / (batch_loss>0).sum(1)\n",
    "            loss_vec.extend(batch_loss.cpu().tolist())\n",
    "        if debug: break\n",
    "        \n",
    "    if debug:        \n",
    "        return {\"loss_vec\": np.array(loss_vec), \n",
    "                \"batch\": batch, \"ylabels\": ylabels, \"out\": out}\n",
    "    else:\n",
    "        return np.array(loss_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eca001cb-01b2-47c0-a301-6b761919bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "Mtest = len(test_idxs)\n",
    "n_split = len(cv_splits)\n",
    "def make_design_iter():\n",
    "    return product([\"cnstr\", \"cslot\", \"vslot\"], # cnstr elements \n",
    "                   [\"raw\", \"shifted\", \"random\"]) # masked type\n",
    "\n",
    "def get_mask_condition(cx_elem, mask_type):\n",
    "    cx_code = \"cx\" if cx_elem == \"cnstr\" else cx_elem\n",
    "    mask_code = \"\" if mask_type == \"raw\" else mask_type+\"-\"\n",
    "    return mask_code+cx_code\n",
    "\n",
    "def print_stat(x):\n",
    "    print(\"M={:.4f}, SD={:.4f} ({} NaNs)\"\n",
    "          .format(np.nanmean(x), np.nanstd(x), np.sum(np.isnan(x))))\n",
    "\n",
    "def make_buffer(n_split=10):\n",
    "    ret = {}\n",
    "    for cx_elem, mtype in make_design_iter():\n",
    "        ret[f\"{cx_elem}_{mtype}\"] = \\\n",
    "            np.zeros((Mtest, n_split))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c31f5-1db7-4bf1-975c-9d27b6c65f7c",
   "metadata": {},
   "source": [
    "## Computing MLM Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bad2bf4a-6757-4dea-88b4-72dec101bfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e390259b1cd422bb6fc698815cc7121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## masked\n",
    "n_condition = len(list(make_design_iter()))\n",
    "losses = make_buffer(1)\n",
    "split_idx = 0\n",
    "train_idxs, test_idxs = cv_splits[split_idx].values()\n",
    "pbar = tqdm(total=n_condition)\n",
    "for cx_elem, mtype in make_design_iter():        \n",
    "    cond_text = f\"{cx_elem}_{mtype}\"\n",
    "    pbar.set_description(f\"{split_idx+1: 2d}. {cond_text}\")\n",
    "    m_cond = get_mask_condition(cx_elem, mtype)        \n",
    "    loss_x = compute_mlm_loss(model, test_idxs, m_cond)        \n",
    "    losses[cond_text][:, split_idx] = loss_x            \n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddceef4d-0823-496d-ae13-351f22154f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cnstr_raw', 4.223324037336069),\n",
       " ('cnstr_shifted', 4.566821129403833),\n",
       " ('cnstr_random', 3.3012096010116547),\n",
       " ('cslot_raw', 2.711533956908393),\n",
       " ('cslot_shifted', 2.326555755823354),\n",
       " ('cslot_random', 2.1128639750665528),\n",
       " ('vslot_raw', 2.682894103944538),\n",
       " ('vslot_shifted', 3.078818427914438),\n",
       " ('vslot_random', 2.7265981398804775)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k,np.mean(x)) for k, x in losses.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9c1bd-4f55-40fd-8ab1-6335ee07b51f",
   "metadata": {},
   "source": [
    "## Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59f7b99e-5359-4eec-a213-796303c0e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "slot_lenc = LabelEncoder()\n",
    "slot_lenc.classes_ = [\"[PAD]\", \"BC\", \"IC\", \"BV\", \"IV\", \"O\"]\n",
    "\n",
    "def sample_varslots(model, train_idxs):\n",
    "    batch_size = 16\n",
    "    max_len = 256\n",
    "    samples = []  \n",
    "    \n",
    "    for i in range(0, len(train_idxs), batch_size):\n",
    "        data_idxs = train_idxs[i:i+batch_size]\n",
    "        batch = batched_text_gan(data, data_idxs)\n",
    "        \n",
    "        real_tokens = tokenizer(batch[\"text\"], return_tensors=\"pt\", \n",
    "                          is_split_into_words=True, padding=True, truncation=True,\n",
    "                          return_token_type_ids=False, return_attention_mask=False, \n",
    "                          max_length=max_len)    \n",
    "        masked_tokens = tokenizer(batch[\"masked\"], return_tensors=\"pt\", \n",
    "                          is_split_into_words=True, padding=True, truncation=True,\n",
    "                          max_length=max_len)    \n",
    "        \n",
    "        slot_tags = [torch.tensor(slot_lenc.transform([\"[PAD]\"] + x + [\"[PAD]\"])) \n",
    "                     for x in batch[\"slot_tags\"]]        \n",
    "        slot_tags = pad_sequence(slot_tags, batch_first=True, padding_value=0)\n",
    "        slot_tags = slot_tags[:, :real_tokens.input_ids.size(1)]                   \n",
    "        \n",
    "        BV_id = slot_lenc.transform([\"BV\"])[0]\n",
    "        IV_id = slot_lenc.transform([\"IV\"])[0]\n",
    "        adv_labels = (slot_tags == BV_id).clone()        \n",
    "        adv_labels = torch.logical_or(adv_labels, slot_tags==IV_id, out=adv_labels)\n",
    "                \n",
    "        batch[\"slot_tags\"] = slot_tags.to(device)\n",
    "        batch[\"real_text\"] = real_tokens.to(device)\n",
    "        batch[\"masked_text\"] = masked_tokens.to(device)\n",
    "        \n",
    "        # generate GAN real/fake labels\n",
    "        gen_labels = torch.full_like(slot_tags, 0)\n",
    "        gen_labels.masked_fill_(adv_labels, 1)\n",
    "        batch[\"gen_labels\"] = gen_labels        \n",
    "        \n",
    "        with torch.no_grad():            \n",
    "            out = model(**masked_tokens)\n",
    "            adv_out = generate_adversarials(batch, out.logits.softmax(axis=2))            \n",
    "            samples.append(adv_out)\n",
    "        break\n",
    "            \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a47085a6-7da3-4c9f-856f-fbd3d7c129d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6598, 4660]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idxs[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ca2e5-8871-4bae-8e90-ee1e100bea59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08996779-8fc9-4621-894d-22a66abf19da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[135, 137,  -1,  -1,  -1,  -1],\n",
       "       [ 13,  15,  -1,  -1,  -1,  -1],\n",
       "       [  3,   5,  -1,  -1,  -1,  -1],\n",
       "       [  0,   1,   2,   4,   5,   6],\n",
       "       [  0,   2,  -1,  -1,  -1,  -1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"mindex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "973a4d6e-23aa-4415-8569-a2f1e76aa458",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41838/2232018637.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": [
    "probs[2, [6,8], :].argsort()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b174c33-1e1b-45a9-9e86-6947692086d6",
   "metadata": {},
   "source": [
    "## Sample variable site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df349fa7-a605-4922-88cd-b30f2264f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conart import gan_utils as gu\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict\n",
    "\n",
    "def sample_site(batch, merge_pair2=False):\n",
    "    masked_tokens = tokenizer(batch[\"masked\"], return_tensors=\"pt\", \n",
    "                          is_split_into_words=True, padding=True, truncation=True,\n",
    "                          max_length=200)  \n",
    "    with torch.no_grad():\n",
    "        masked_tokens = masked_tokens.to(device)\n",
    "        out = model(**masked_tokens)\n",
    "        probs = F.log_softmax(out.logits, dim=2).cpu().numpy()\n",
    "    \n",
    "    mindex = batch[\"mindex\"]\n",
    "    \n",
    "    samples = []\n",
    "    for i in range(mindex.shape[0]):\n",
    "        mindex_x = mindex[i,:]+1\n",
    "        mindex_x = mindex_x[mindex_x>0]\n",
    "        probs_x = probs[i, mindex_x, :]\n",
    "        if len(mindex_x) == 2 and merge_pair2:            \n",
    "            probs_sum = probs[i, mindex_x, :].sum(0)\n",
    "            arg_x = probs_sum.argsort()[::-1][:10]            \n",
    "            samples.append({\"ids\": arg_x, \n",
    "                            \"probs\": probs_sum[arg_x]})\n",
    "        else:\n",
    "            arg_x = probs[i, mindex_x, :].argsort(axis=1)[:, ::-1][:, :10]\n",
    "            prob_arg = probs[i, mindex_x, :].argsort(axis=1)[:, ::-1][:, :10]\n",
    "            samples.append({\"ids\": arg_x, \n",
    "                            \"probs\": np.take_along_axis(probs_x, arg_x, 1)})\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f8e0380-0dc5-47a1-b6f3-e6cac5f87720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_guesses(test_idx):\n",
    "    batch = batched_text(data, test_idxs[test_idx:test_idx+1], 'vslot')\n",
    "    print(\"Masked\", \"\".join(batch[\"masked\"][0]))\n",
    "    print(\"Origin\", \"\".join(batch[\"text\"][0]))\n",
    "    samples = sample_site(batch)[0]\n",
    "    prob_sort = samples[\"probs\"].sum(0).argsort()\n",
    "    print(\"Model (separated): \", tokenizer.batch_decode(samples[\"ids\"]))\n",
    "    samples = sample_site(batch, merge_pair2=True)[0]\n",
    "    prob_sort = samples[\"probs\"].sum(0).argsort()\n",
    "    print(\"Model (merged): \", tokenizer.batch_decode(samples[\"ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3e9a2be-7fb3-40a7-a4a3-d58479fd2c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked 這麼熟門熟路吳先生是誰吳先生很懂喔我連[MASK]都沒[MASK]過他都知道\n",
      "Origin 這麼熟門熟路吳先生是誰吳先生很懂喔我連聽都沒聽過他都知道\n",
      "Model (separated):  ['聽 看 查 想 見 去 問 找 提 講', '聽 見 看 想 問 查 找 去 提 講']\n",
      "Model (merged):  ['聽', '看', '見', '想', '查', '問', '去', '找', '提', '講']\n"
     ]
    }
   ],
   "source": [
    "generate_guesses(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ce1eab3-7dd7-4eda-b822-68767a6d447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked 原本買小的但[MASK]一[MASK]有時候看鏡子整體的手感覺手錶好小\n",
      "Origin 原本買小的但戴一戴有時候看鏡子整體的手感覺手錶好小\n",
      "Model (separated):  ['洗 想 算 買 修 看 用 逛 摸 動', '想 洗 算 買 修 動 摸 看 用 玩']\n",
      "Model (merged):  ['想', '洗', '算', '買', '修', '看', '動', '用', '摸', '玩']\n"
     ]
    }
   ],
   "source": [
    "generate_guesses(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fcc7dae-52c8-4ace-a6dc-2082af2ac17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked 運動強度沒有太高圖個[MASK]一[MASK]\n",
      "Origin 運動強度沒有太高圖個動一動\n",
      "Model (separated):  ['升 緩 加 動 忍 玩 瘦 練 晃 撐', '緩 忍 升 加 動 醒 笑 想 練 晃']\n",
      "Model (merged):  ['升', '緩', '忍', '加', '動', '醒', '練', '晃', '玩', '想']\n"
     ]
    }
   ],
   "source": [
    "generate_guesses(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32003f-cb27-4dd7-9436-db7a64cd0647",
   "metadata": {},
   "source": [
    "## Output pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba799d8-1901-4b97-82bd-036d09e10c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

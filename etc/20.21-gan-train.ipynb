{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947f7331-799e-457e-9b05-f707abd298e9",
   "metadata": {},
   "source": [
    "## GAN training for MLM\n",
    "* Inputs:\n",
    "  * raw data: `../data/raw_cx_data.json` (10.01)\n",
    "  * CV splits: `../data/cv_splits_10.json` (10.01)\n",
    "* Outputs:\n",
    "  * (none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a002c61d-99e2-4e20-a1fd-8d739a3afd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b8acef-ef12-40d0-9d27-fde1bd8b7f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from hashlib import sha256\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import chain\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss, NLLLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import MeanMetric\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizerFast, PreTrainedModel, PretrainedConfig\n",
    "from transformers import BertPreTrainedModel, BertModel, BertForMaskedLM, BertForTokenClassification\n",
    "from import_conart import conart\n",
    "from conart.mlm_masks import batched_text_gan\n",
    "from conart import gan_utils as gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b571992-7a21-4000-80e1-bab0f9d8babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") \\\n",
    "         if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a62670-63f0-45f2-98b1-f8aa45b47866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../data/raw_cx_data.json\"\n",
    "with open(data_path, \"r\", encoding=\"UTF-8\") as fin:\n",
    "    data = json.load(fin)\n",
    "## Check data is the same\n",
    "h = sha256()\n",
    "h.update(pickle.dumps(data))\n",
    "data_hash = h.digest().hex()[:6]\n",
    "assert data_hash == \"4063b4\"\n",
    "len(data) # should be 11642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7556dcc-1a31-40c8-b142-9a41f7e4ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read cv splits\n",
    "with open(\"../data/cv_splits_10.json\", \"r\") as fin:\n",
    "    cv_splits = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46be44f6-5e00-4f5c-ac66-aec6f4441879",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce38a19-96c3-44c8-9985-a513758804be",
   "metadata": {},
   "source": [
    "## Checking input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d978cb6d-17eb-45b4-95a9-ece138f1f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, test_idxs = cv_splits[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1179ed-8332-46d9-99af-e5ca98da2f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'board': 'BabyMother',\n",
       " 'text': ['再', '擠', '也', '擠', '不', '出來', '了'],\n",
       " 'cnstr': ['O', 'BX', 'IX', 'IX', 'IX', 'IX', 'O'],\n",
       " 'slot': ['O', 'BV', 'BC', 'BV', 'BC', 'BV', 'O'],\n",
       " 'cnstr_form': ['v', '也', 'v', '不', 'X'],\n",
       " 'cnstr_example': ['擠', '也', '擠', '不', '出來']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = data[1211]\n",
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a826fa-19ed-488b-b4dd-bae771ec8094",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39107d4-75f8-46fe-9654-104ba544322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs_ds = TensorDataset(torch.tensor(train_idxs))\n",
    "test_idxs_ds = TensorDataset(torch.tensor(test_idxs))\n",
    "cx_lenc = LabelEncoder()\n",
    "cx_lenc.classes_ = [\"[PAD]\", \"BX\", \"IX\", \"O\"]\n",
    "slot_lenc = LabelEncoder()\n",
    "slot_lenc.classes_ = [\"[PAD]\", \"BC\", \"IC\", \"BV\", \"IV\", \"O\"]\n",
    "adv_lenc = LabelEncoder()\n",
    "adv_lenc.classes_ = [\"fake\", \"real\"]\n",
    "BV_id = slot_lenc.transform([\"BV\"])[0]\n",
    "IV_id = slot_lenc.transform([\"IV\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a45f91-251c-4579-9f55-8d45a3efe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_collate_fn(X, data, cx_lenc, slot_lenc, device=\"cpu\"):  \n",
    "    idxs = [x[0].item() for x in X]\n",
    "    batch = batched_text_gan(data, idxs)\n",
    "    \n",
    "    max_len = 200\n",
    "    real_tokens = tokenizer(batch[\"text\"], return_tensors=\"pt\", \n",
    "                          is_split_into_words=True, padding=True, truncation=True,\n",
    "                          return_token_type_ids=False, return_attention_mask=False, \n",
    "                          max_length=max_len)    \n",
    "    masked_tokens = tokenizer(batch[\"masked\"], return_tensors=\"pt\", \n",
    "                          is_split_into_words=True, padding=True, truncation=True,\n",
    "                          max_length=max_len)    \n",
    "    \n",
    "    cx_tags = [torch.tensor(cx_lenc.transform([\"[PAD]\"] + x + [\"[PAD]\"]))\n",
    "               for x in batch[\"cx_tags\"]]\n",
    "    cx_tags = pad_sequence(cx_tags, batch_first=True, padding_value=0)\n",
    "    cx_tags = cx_tags[:, :real_tokens.input_ids.size(1)]\n",
    "    \n",
    "    slot_tags = [torch.tensor(slot_lenc.transform([\"[PAD]\"] + x + [\"[PAD]\"])) \n",
    "                 for x in batch[\"slot_tags\"]]        \n",
    "    slot_tags = pad_sequence(slot_tags, batch_first=True, padding_value=0)\n",
    "    slot_tags = slot_tags[:, :real_tokens.input_ids.size(1)]\n",
    "    batch[\"cx_tags\"] = cx_tags.to(device)\n",
    "    batch[\"slot_tags\"] = slot_tags.to(device)\n",
    "    batch[\"real_text\"] = real_tokens.to(device)\n",
    "    batch[\"masked_text\"] = masked_tokens.to(device)\n",
    "    batch.update(gu.make_gendcr_labels(batch, adv_ids=[BV_id, IV_id]))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a4e27d8-5041-478a-a8b2-f7f650a70202",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = gan_collate_fn([test_idxs_ds[202], test_idxs_ds[203]], data, cx_lenc, slot_lenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "407f60b1-db74-4b14-940b-8821d1c1a4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'masked',\n",
       " 'cx_tags',\n",
       " 'slot_tags',\n",
       " 'real_text',\n",
       " 'masked_text',\n",
       " 'gen_labels']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bb.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc6390e7-1e7e-4e85-8f7c-eee3f657cac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS],0', '也,5', '是,5', '直,5', '接,5', '退,3', '一,1', '退,3', '海,5', '闊,5', '天,5', '空,5', '了,5', '[SEP],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0']\n",
      "['[CLS],0', '也,3', '是,3', '直,3', '接,3', '退,1', '一,2', '退,2', '海,3', '闊,3', '天,3', '空,3', '了,3', '[SEP],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0']\n",
      "['[CLS],-100', '也,-100', '是,-100', '直,-100', '接,-100', '退,1', '一,-100', '退,1', '海,-100', '闊,-100', '天,-100', '空,-100', '了,-100', '[SEP],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100']\n"
     ]
    }
   ],
   "source": [
    "## visual check\n",
    "print([f\"{a},{b.item()}\" for a,b in zip(tokenizer.convert_ids_to_tokens(bb[\"real_text\"].input_ids[0]), bb[\"slot_tags\"][0])])\n",
    "print([f\"{a},{b.item()}\" for a,b in zip(tokenizer.convert_ids_to_tokens(bb[\"real_text\"].input_ids[0]), bb[\"cx_tags\"][0])])\n",
    "print([f\"{a},{b.item()}\" for a,b in zip(tokenizer.convert_ids_to_tokens(bb[\"real_text\"].input_ids[0]), bb[\"gen_labels\"][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f971bc1-ea16-466b-8318-a5ddfe2b2778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100,    1, -100,    1, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100,    1,    1, -100,\n",
       "            1,    1, -100,    1, -100]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb[\"gen_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f7ac3-3fbd-4462-8ee6-91dca4cd81fc",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47553e9c-1166-4afc-b6ad-e367326add95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConartModelDonut(PreTrainedModel):\n",
    "    def __init__(self, config, bertD_ckpt, bertG_ckpt):        \n",
    "        super(ConartModelDonut, self).__init__(config)\n",
    "        # inherit self.bert, self.cls (lm head) from super()              \n",
    "        self.bertD = BertForTokenClassification.from_pretrained(bertD_ckpt, num_labels=1)\n",
    "        self.bertG = BertForMaskedLM.from_pretrained(bertG_ckpt)        \n",
    "    \n",
    "    def G_params(self):\n",
    "        return self.bertG.parameters()\n",
    "    \n",
    "    def D_params(self):\n",
    "        return self.bertD.parameters()\n",
    "    \n",
    "    def forward_G(self, X, labels=None):\n",
    "        tokens = X[\"masked_text\"]\n",
    "        out = self.bertG(**tokens, labels=labels, return_dict=True)        \n",
    "        lmlogits = out.logits\n",
    "        lmprobs = lmlogits.softmax(dim=2)\n",
    "        ret = {\"lm_probs\": lmprobs, \"logits\": lmlogits, \n",
    "               \"mlm_loss\": out.loss}        \n",
    "        return ret\n",
    "        \n",
    "    \n",
    "    def forward_D(self, X, labels=None):\n",
    "        tokens = X[\"masked_text\"]\n",
    "        out = self.bertD(**tokens, return_dict=True)\n",
    "        tok_logits = out.logits\n",
    "        ret = {\"tok_logits\": tok_logits}\n",
    "        \n",
    "        if labels is not None:        \n",
    "            real_loss = -torch.mean(tok_logits[labels==1])\n",
    "            fake_loss = torch.mean(tok_logits[labels==0])\n",
    "            adv_loss = real_loss + fake_loss\n",
    "            ret[\"adv_loss\"] = adv_loss\n",
    "        return ret\n",
    "    \n",
    "    def forward(self, X):\n",
    "        tokens = X[\"masked_text\"]\n",
    "        cx_tags = X[\"cx_tags\"]\n",
    "        slot_tags = X[\"slot_tags\"]\n",
    "        bert_out = self.bert(**tokens, return_dict=True)\n",
    "        return bert_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f1f4ee-4ba4-4190-8966-0aee5b462088",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check adversarial samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8c3ba-54ab-48e2-a0ab-9374a21c1814",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31ec668b-2eda-4297-a768-a0ae856d5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = lambda x: gan_collate_fn(x, data, cx_lenc, slot_lenc, device)\n",
    "debug_loader = DataLoader(test_idxs_ds, batch_size=2, shuffle=False, \n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "827d7764-c216-4924-b193-970d2af3538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckiplab/bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = BertForMaskedLM.from_pretrained('bert-base-chinese')\n",
    "config = PretrainedConfig()\n",
    "model = ConartModelDonut(config, \"ckiplab/bert-base-chinese\", \"ckiplab/bert-base-chinese\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86d6e4ce-fa86-4c73-b575-8cbe7bc721db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm_probs': tensor([[[1.3438e-08, 2.0852e-08, 2.4144e-08,  ..., 2.8087e-07,\n",
       "           4.3796e-07, 4.2810e-08],\n",
       "          [1.7458e-18, 1.3219e-17, 2.1442e-17,  ..., 3.6188e-15,\n",
       "           3.1057e-16, 8.1137e-17],\n",
       "          [2.5644e-23, 1.1808e-21, 2.4968e-22,  ..., 1.1142e-20,\n",
       "           3.2149e-20, 6.1896e-22],\n",
       "          ...,\n",
       "          [1.0755e-19, 1.6767e-19, 2.0689e-18,  ..., 2.9581e-16,\n",
       "           4.0828e-17, 4.9804e-18],\n",
       "          [1.7951e-15, 1.0347e-14, 1.5952e-14,  ..., 4.8796e-13,\n",
       "           1.3940e-13, 4.5419e-14],\n",
       "          [1.3372e-08, 2.0787e-08, 2.4046e-08,  ..., 2.8081e-07,\n",
       "           4.3771e-07, 4.2651e-08]],\n",
       " \n",
       "         [[2.8199e-08, 5.6591e-08, 4.1160e-08,  ..., 5.2027e-07,\n",
       "           3.3007e-07, 1.5627e-07],\n",
       "          [3.6332e-14, 5.8456e-14, 1.0368e-13,  ..., 2.0944e-12,\n",
       "           3.0220e-14, 9.4347e-13],\n",
       "          [2.0888e-11, 2.1918e-11, 1.3978e-11,  ..., 4.3487e-11,\n",
       "           2.0442e-12, 5.0283e-10],\n",
       "          ...,\n",
       "          [4.0348e-10, 3.0904e-09, 3.8405e-09,  ..., 7.1305e-08,\n",
       "           2.2333e-07, 2.2525e-08],\n",
       "          [1.0148e-09, 6.2152e-09, 8.7631e-09,  ..., 1.1523e-07,\n",
       "           2.5679e-07, 9.5672e-08],\n",
       "          [3.2027e-09, 1.5432e-08, 1.8695e-08,  ..., 3.0905e-07,\n",
       "           5.2431e-07, 3.7652e-07]]], device='cuda:0',\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " 'logits': tensor([[[ -7.8838,  -7.4445,  -7.2979,  ...,  -4.8440,  -4.3998,  -6.7251],\n",
       "          [-13.1779, -11.1535, -10.6698,  ...,  -5.5413,  -7.9967,  -9.3390],\n",
       "          [-14.7936, -10.9640, -12.5178,  ...,  -8.7195,  -7.6598, -11.6099],\n",
       "          ...,\n",
       "          [-10.8994, -10.4553,  -7.9425,  ...,  -2.9799,  -4.9602,  -7.0641],\n",
       "          [ -8.4700,  -6.7184,  -6.2855,  ...,  -2.8649,  -4.1177,  -5.2392],\n",
       "          [ -7.8857,  -7.4445,  -7.2989,  ...,  -4.8412,  -4.3973,  -6.7258]],\n",
       " \n",
       "         [[ -7.8314,  -7.1348,  -7.4532,  ...,  -4.9163,  -5.3714,  -6.1191],\n",
       "          [ -8.1710,  -7.6954,  -7.1223,  ...,  -4.1166,  -8.3552,  -4.9141],\n",
       "          [ -7.0568,  -7.0086,  -7.4584,  ...,  -6.3235,  -9.3809,  -3.8757],\n",
       "          ...,\n",
       "          [-10.0263,  -7.9904,  -7.7731,  ...,  -4.8517,  -3.7101,  -6.0041],\n",
       "          [ -9.4095,  -7.5972,  -7.2537,  ...,  -4.6773,  -3.8760,  -4.8633],\n",
       "          [ -8.7440,  -7.1716,  -6.9798,  ...,  -4.1745,  -3.6460,  -3.9771]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'mlm_loss': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = next(iter(debug_loader))\n",
    "model.forward_G(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e4985a8-22a5-464a-ad1f-54cae7d50710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]我覺得這位護理師\u001b[31m[MASK](\u001b[4m錯\u001b[0;31m)\u001b[0m就\u001b[31m[MASK](\u001b[4m錯\u001b[0;31m)\u001b[0m在\u001b[31m[MASK](\u001b[4m抽\u001b[0;31m)\u001b[0m了一位不理性\n",
      "[CLS]1km補給品\u001b[31m[MASK](\u001b[4m買\u001b[0;31m)\u001b[0m一\u001b[31m[MASK](\u001b[4m買\u001b[0;31m)\u001b[0m窩著先看這幾天發展比\n"
     ]
    }
   ],
   "source": [
    "masked_ids = bb[\"gen_labels\"].masked_scatter(bb[\"gen_labels\"]==1, bb[\"real_text\"].input_ids)\n",
    "lm_out = model.forward_G(bb, masked_ids)\n",
    "gu.visualize_gen(bb, masked_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a53be522-4874-4249-9086-0e3bbbb5ff26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.0936, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_out[\"mlm_loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14577bbb-f6b8-4aad-9951-56a044866442",
   "metadata": {},
   "source": [
    "### Adversarial sample & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "535b9689-b5ea-4c7e-a41f-37b3c6f853ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]我覺得這位護理師\u001b[31m不(\u001b[4m錯\u001b[0;31m)\u001b[0m就\u001b[31m像(\u001b[4m錯\u001b[0;31m)\u001b[0m在抽了一位不理性\n",
      "[CLS]1km補給品\u001b[31m第(\u001b[4m買\u001b[0;31m)\u001b[0m一\u001b[31m窩(\u001b[4m買\u001b[0;31m)\u001b[0m窩著先看這幾天發展比\n",
      "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 50, 1])\n"
     ]
    }
   ],
   "source": [
    "lm_probs = model.forward_G(bb)[\"lm_probs\"]\n",
    "adv_out = gu.generate_adversarials(bb, lm_probs)\n",
    "gu.visualize_adv(adv_out, tokenizer)\n",
    "tok_out = model.forward_D(bb, labels=adv_out[\"dcr_labels\"])\n",
    "print(tok_out[\"adv_loss\"])\n",
    "print(tok_out[\"tok_logits\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3df0a8b6-58e3-43f2-bc9e-41edf05e7e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2769, 6221, 2533, 6857,  855, 6362, 4415, 2374,  679, 2218, 1008,\n",
       "         1762, 2853,  749,  671,  855,  679, 4415, 2595, 2097, 2044, 4638, 6117,\n",
       "         1416,  872, 4534,  678, 1377,  809,  679, 6206, 1526, 1962, 1962, 6656,\n",
       "         1961, 3978, 6858, 6313, 1961, 2994,  782, 2853, 1416, 3559,  677,  872,\n",
       "         1962,  102],\n",
       "        [ 101,  122,  153,  155, 6171, 5183, 1501, 5018,  671, 4979, 4979, 5865,\n",
       "         1044, 4692, 6857, 2407, 1921, 4634, 2245, 3683, 6733, 4952, 5018,  671,\n",
       "         3613, 6882, 2399, 4522, 1762, 7770, 7413,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_out[\"adv_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d1acb-afb8-4099-b13f-76be0b6893ad",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "868a2a77-7b8e-4b0b-8d2b-aec9f3a4282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  16\n",
      "Training dataset: 10477\n",
      "Testing dataset: 1165\n"
     ]
    }
   ],
   "source": [
    "collate_fn = lambda x: gan_collate_fn(x, data, cx_lenc, slot_lenc, device)\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_idxs_ds, batch_size=batch_size, shuffle=True, \n",
    "                         collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_idxs_ds, batch_size=batch_size, shuffle=True, \n",
    "                         collate_fn=collate_fn)\n",
    "print(\"batch size: \", batch_size)\n",
    "print(\"Training dataset:\", len(train_idxs_ds))\n",
    "print(\"Testing dataset:\", len(test_idxs_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006dc167-8aa9-4a56-a47e-df6a47f5421d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckiplab/bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = BertForMaskedLM.from_pretrained('bert-base-chinese')\n",
    "config = PretrainedConfig()\n",
    "model = ConartModelDonut(config, \"ckiplab/bert-base-chinese\", \"ckiplab/bert-base-chinese\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae073c3c-2d39-4326-b33d-b6ee269af360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f9ae080b51429ca9e6b405bdd906f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c68ba2c145417ab4f4f10b6ca3275b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05720d381b894f999d5f92a35a3bf38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train generator\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "writer = SummaryWriter(log_dir=\"../data/tb_logs/train_adv_01\", \n",
    "                       comment='epoch=6, lr=1e-5, with linear scheduler')\n",
    "\n",
    "to_train_G = False\n",
    "\n",
    "data_loader = train_loader\n",
    "n_epoch = 3\n",
    "optim_G = optim.AdamW(model.G_params(), lr=1e-5)\n",
    "optim_D = optim.AdamW(model.D_params(), lr=1e-5)\n",
    "scheduler_G = get_linear_schedule_with_warmup(optim_G, 50, len(data_loader)*n_epoch)\n",
    "scheduler_D = get_linear_schedule_with_warmup(optim_D, 50, len(data_loader)*n_epoch)\n",
    "# optim_dcr_cls = optim.AdamW(model.tok_cls.parameters(), lr=1e-4)\n",
    "\n",
    "mlm_loss_epoch = MeanMetric()\n",
    "dcr_loss_epoch = MeanMetric()\n",
    "\n",
    "iter_idx = 0\n",
    "for epoch_i in range(n_epoch):\n",
    "       \n",
    "    for batch_idx, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        #  mlm prediction\n",
    "        masked_ids = batch[\"gen_labels\"].masked_scatter(\n",
    "                        batch[\"gen_labels\"]==1, batch[\"real_text\"].input_ids)\n",
    "        lm_out = model.forward_G(batch, masked_ids)\n",
    "        mlm_loss = lm_out[\"mlm_loss\"]\n",
    "        writer.add_scalar('mlm_loss', mlm_loss.item(), iter_idx)\n",
    "        mlm_loss_epoch.update(mlm_loss.item())    \n",
    "                \n",
    "        # generate adversarial samples\n",
    "        lm_probs = model.forward_G(batch)[\"lm_probs\"]\n",
    "        adv_out = gu.generate_adversarials(batch, lm_probs)\n",
    "        \n",
    "        if batch_idx % 3 == 0:\n",
    "            to_train_G = True\n",
    "        else:\n",
    "            to_train_G = False\n",
    "        \n",
    "        if to_train_G:\n",
    "            # train generator\n",
    "            optim_G.zero_grad()\n",
    "            mlm_loss.backward()\n",
    "            optim_G.step()\n",
    "            scheduler_G.step()\n",
    "\n",
    "\n",
    "        # compute adv loss\n",
    "        tok_out = model.forward_D(batch, labels=adv_out[\"dcr_labels\"])\n",
    "        adv_loss = tok_out[\"adv_loss\"]\n",
    "        writer.add_scalar(\"adv_loss\", adv_loss.item(), iter_idx)\n",
    "        dcr_loss_epoch.update(adv_loss.item()) \n",
    "                    \n",
    "        # train discriminator\n",
    "        optim_D.zero_grad()\n",
    "        adv_loss.backward()\n",
    "        optim_D.step()\n",
    "        scheduler_D.step()\n",
    "\n",
    "        iter_idx += 1        \n",
    "    writer.add_scalar(\"dcr_loss_epoch\", dcr_loss_epoch.compute(), epoch_i)\n",
    "    writer.add_scalar(\"mlm_loss_epoch\", mlm_loss_epoch.compute(), epoch_i)\n",
    "    dcr_loss_epoch.reset()\n",
    "    mlm_loss_epoch.reset()\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d952dc6c-c933-402e-ae2e-0bbe9f1ec356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../data/models/donut_adv_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f2c7b-8a31-4995-8b6b-fdbd6e174d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternating D/G"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

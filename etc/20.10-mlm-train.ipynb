{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947f7331-799e-457e-9b05-f707abd298e9",
   "metadata": {},
   "source": [
    "## GAN training for MLM\n",
    "* Inputs:\n",
    "  * raw data: `../data/raw_cx_data.json` (10.01)\n",
    "  * CV splits: `../data/cv_splits_10.json` (10.01)\n",
    "* Outputs:\n",
    "  * (none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a002c61d-99e2-4e20-a1fd-8d739a3afd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b8acef-ef12-40d0-9d27-fde1bd8b7f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from hashlib import sha256\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import chain\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss, NLLLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import MeanMetric\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertPreTrainedModel, BertModel, BertForMaskedLM\n",
    "from import_conart import conart\n",
    "from conart.mlm_masks import batched_text_gan\n",
    "from conart import gan_utils as gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b571992-7a21-4000-80e1-bab0f9d8babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") \\\n",
    "         if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a62670-63f0-45f2-98b1-f8aa45b47866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../data/raw_cx_data.json\"\n",
    "with open(data_path, \"r\", encoding=\"UTF-8\") as fin:\n",
    "    data = json.load(fin)\n",
    "## Check data is the same\n",
    "h = sha256()\n",
    "h.update(pickle.dumps(data))\n",
    "data_hash = h.digest().hex()[:6]\n",
    "assert data_hash == \"4063b4\"\n",
    "len(data) # should be 11642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7556dcc-1a31-40c8-b142-9a41f7e4ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read cv splits\n",
    "with open(\"../data/cv_splits_10.json\", \"r\") as fin:\n",
    "    cv_splits = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46be44f6-5e00-4f5c-ac66-aec6f4441879",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce38a19-96c3-44c8-9985-a513758804be",
   "metadata": {},
   "source": [
    "## Checking input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d978cb6d-17eb-45b4-95a9-ece138f1f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, test_idxs = cv_splits[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1179ed-8332-46d9-99af-e5ca98da2f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'board': 'BabyMother',\n",
       " 'text': ['再', '擠', '也', '擠', '不', '出來', '了'],\n",
       " 'cnstr': ['O', 'BX', 'IX', 'IX', 'IX', 'IX', 'O'],\n",
       " 'slot': ['O', 'BV', 'BC', 'BV', 'BC', 'BV', 'O'],\n",
       " 'cnstr_form': ['v', '也', 'v', '不', 'X'],\n",
       " 'cnstr_example': ['擠', '也', '擠', '不', '出來']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = data[1211]\n",
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a826fa-19ed-488b-b4dd-bae771ec8094",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39107d4-75f8-46fe-9654-104ba544322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs_ds = TensorDataset(torch.tensor(train_idxs))\n",
    "test_idxs_ds = TensorDataset(torch.tensor(test_idxs))\n",
    "cx_lenc = LabelEncoder()\n",
    "cx_lenc.classes_ = [\"[PAD]\", \"BX\", \"IX\", \"O\"]\n",
    "slot_lenc = LabelEncoder()\n",
    "slot_lenc.classes_ = [\"[PAD]\", \"BC\", \"IC\", \"BV\", \"IV\", \"O\"]\n",
    "adv_lenc = LabelEncoder()\n",
    "adv_lenc.classes_ = [\"fake\", \"real\"]\n",
    "BV_id = slot_lenc.transform([\"BV\"])[0]\n",
    "IV_id = slot_lenc.transform([\"IV\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a45f91-251c-4579-9f55-8d45a3efe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_collate_fn(X, data, cx_lenc, slot_lenc, device=\"cpu\"):  \n",
    "    idxs = [x[0].item() for x in X]\n",
    "    batch = batched_text_gan(data, idxs)\n",
    "    \n",
    "    max_len = 200\n",
    "    real_tokens = tokenizer(batch[\"text\"], return_tensors=\"pt\", \n",
    "                          is_split_into_words=True, padding=True, truncation=True,\n",
    "                          return_token_type_ids=False, return_attention_mask=False, \n",
    "                          max_length=max_len)    \n",
    "    masked_tokens = tokenizer(batch[\"masked\"], return_tensors=\"pt\", \n",
    "                          is_split_into_words=True, padding=True, truncation=True,\n",
    "                          max_length=max_len)    \n",
    "    \n",
    "    cx_tags = [torch.tensor(cx_lenc.transform([\"[PAD]\"] + x + [\"[PAD]\"]))\n",
    "               for x in batch[\"cx_tags\"]]\n",
    "    cx_tags = pad_sequence(cx_tags, batch_first=True, padding_value=0)\n",
    "    cx_tags = cx_tags[:, :real_tokens.input_ids.size(1)]\n",
    "    \n",
    "    slot_tags = [torch.tensor(slot_lenc.transform([\"[PAD]\"] + x + [\"[PAD]\"])) \n",
    "                 for x in batch[\"slot_tags\"]]        \n",
    "    slot_tags = pad_sequence(slot_tags, batch_first=True, padding_value=0)\n",
    "    slot_tags = slot_tags[:, :real_tokens.input_ids.size(1)]\n",
    "    batch[\"cx_tags\"] = cx_tags.to(device)\n",
    "    batch[\"slot_tags\"] = slot_tags.to(device)\n",
    "    batch[\"real_text\"] = real_tokens.to(device)\n",
    "    batch[\"masked_text\"] = masked_tokens.to(device)\n",
    "    batch.update(gu.make_gendcr_labels(batch, adv_ids=[BV_id, IV_id]))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a4e27d8-5041-478a-a8b2-f7f650a70202",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = gan_collate_fn([test_idxs_ds[202], test_idxs_ds[203]], data, cx_lenc, slot_lenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "407f60b1-db74-4b14-940b-8821d1c1a4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'masked',\n",
       " 'cx_tags',\n",
       " 'slot_tags',\n",
       " 'real_text',\n",
       " 'masked_text',\n",
       " 'gen_labels']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bb.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc6390e7-1e7e-4e85-8f7c-eee3f657cac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS],0', '也,5', '是,5', '直,5', '接,5', '退,3', '一,1', '退,3', '海,5', '闊,5', '天,5', '空,5', '了,5', '[SEP],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0']\n",
      "['[CLS],0', '也,3', '是,3', '直,3', '接,3', '退,1', '一,2', '退,2', '海,3', '闊,3', '天,3', '空,3', '了,3', '[SEP],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0']\n",
      "['[CLS],-100', '也,-100', '是,-100', '直,-100', '接,-100', '退,1', '一,-100', '退,1', '海,-100', '闊,-100', '天,-100', '空,-100', '了,-100', '[SEP],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100', '[PAD],-100']\n"
     ]
    }
   ],
   "source": [
    "## visual check\n",
    "print([f\"{a},{b.item()}\" for a,b in zip(tokenizer.convert_ids_to_tokens(bb[\"real_text\"].input_ids[0]), bb[\"slot_tags\"][0])])\n",
    "print([f\"{a},{b.item()}\" for a,b in zip(tokenizer.convert_ids_to_tokens(bb[\"real_text\"].input_ids[0]), bb[\"cx_tags\"][0])])\n",
    "print([f\"{a},{b.item()}\" for a,b in zip(tokenizer.convert_ids_to_tokens(bb[\"real_text\"].input_ids[0]), bb[\"gen_labels\"][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f971bc1-ea16-466b-8318-a5ddfe2b2778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100,    1, -100,    1, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100,    1,    1, -100,\n",
       "            1,    1, -100,    1, -100]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb[\"gen_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f7ac3-3fbd-4462-8ee6-91dca4cd81fc",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47553e9c-1166-4afc-b6ad-e367326add95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConartModelApricot(BertForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super(ConartModelApricot, self).__init__(config)              \n",
    "    \n",
    "    def forward(self, X, labels=None):\n",
    "        tokens = X[\"masked_text\"]        \n",
    "        out = super().forward(**tokens, labels=labels, return_dict=True)             \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f1f4ee-4ba4-4190-8966-0aee5b462088",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check adversarial samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8c3ba-54ab-48e2-a0ab-9374a21c1814",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31ec668b-2eda-4297-a768-a0ae856d5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = lambda x: gan_collate_fn(x, data, cx_lenc, slot_lenc, device)\n",
    "debug_loader = DataLoader(test_idxs_ds, batch_size=2, shuffle=False, \n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "827d7764-c216-4924-b193-970d2af3538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForMaskedLM.from_pretrained('bert-base-chinese')\n",
    "model = ConartModelApricot.from_pretrained(\"ckiplab/bert-base-chinese\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e4985a8-22a5-464a-ad1f-54cae7d50710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]我覺得這位護理師\u001b[31m[MASK](\u001b[4m錯\u001b[0;31m)\u001b[0m就\u001b[31m[MASK](\u001b[4m錯\u001b[0;31m)\u001b[0m在\u001b[31m[MASK](\u001b[4m抽\u001b[0;31m)\u001b[0m了一位不理性\n",
      "[CLS]1km補給品\u001b[31m[MASK](\u001b[4m買\u001b[0;31m)\u001b[0m一\u001b[31m[MASK](\u001b[4m買\u001b[0;31m)\u001b[0m窩著先看這幾天發展比\n"
     ]
    }
   ],
   "source": [
    "bb = next(iter(debug_loader))\n",
    "masked_ids = bb[\"gen_labels\"].masked_scatter(bb[\"gen_labels\"]==1, bb[\"real_text\"].input_ids)\n",
    "lm_out = model.forward(bb, masked_ids)\n",
    "gu.visualize_gen(bb, masked_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a53be522-4874-4249-9086-0e3bbbb5ff26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.0936, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_out.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d1acb-afb8-4099-b13f-76be0b6893ad",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "868a2a77-7b8e-4b0b-8d2b-aec9f3a4282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  16\n",
      "Training dataset: 10477\n",
      "Testing dataset: 1165\n"
     ]
    }
   ],
   "source": [
    "collate_fn = lambda x: gan_collate_fn(x, data, cx_lenc, slot_lenc, device)\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_idxs_ds, batch_size=batch_size, shuffle=True, \n",
    "                         collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_idxs_ds, batch_size=batch_size, shuffle=True, \n",
    "                         collate_fn=collate_fn)\n",
    "print(\"batch size: \", batch_size)\n",
    "print(\"Training dataset:\", len(train_idxs_ds))\n",
    "print(\"Testing dataset:\", len(test_idxs_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "006dc167-8aa9-4a56-a47e-df6a47f5421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForMaskedLM.from_pretrained('bert-base-chinese')\n",
    "model = ConartModelApricot.from_pretrained(\"ckiplab/bert-base-chinese\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae073c3c-2d39-4326-b33d-b6ee269af360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effb9dfaf4c84b86b11616bdf3a28f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23527c876094a688aafdf5bdd1eb8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train generator\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "writer = SummaryWriter(log_dir=\"../data/tb_logs/train_mlm_03\", \n",
    "                       comment='epoch=5, lr=1e-4, with linear scheduler')\n",
    "\n",
    "to_train_lm = False\n",
    "\n",
    "optim_G = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "data_loader = train_loader\n",
    "n_epoch = 2\n",
    "scheduler_G = get_linear_schedule_with_warmup(optim_G, 50, len(data_loader)*n_epoch)\n",
    "\n",
    "lm_loss_epoch = MeanMetric()\n",
    "\n",
    "iter_idx = 0\n",
    "for epoch_i in range(n_epoch):       \n",
    "    for batch_idx, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        #  mlm prediction\n",
    "        # masked_ids = batch[\"gen_labels\"].masked_scatter(\n",
    "        #                 batch[\"gen_labels\"]==1, batch[\"real_text\"].input_ids)\n",
    "        lm_out = model.forward(batch, batch[\"real_text\"].input_ids)\n",
    "        lm_loss = lm_out.loss\n",
    "        writer.add_scalar('lm_loss', lm_loss.item(), iter_idx)\n",
    "        lm_loss_epoch.update(lm_loss.item())\n",
    "        \n",
    "        # train generator\n",
    "        optim_G.zero_grad()\n",
    "        lm_loss.backward()\n",
    "        optim_G.step()\n",
    "        scheduler_G.step()\n",
    "     \n",
    "        iter_idx += 1        \n",
    "    writer.add_scalar(\"lm_loss_epoch\", lm_loss_epoch.compute(), epoch_i)\n",
    "    lm_loss_epoch.reset()\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d952dc6c-c933-402e-ae2e-0bbe9f1ec356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../data/models/apricot_mlm_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f2c7b-8a31-4995-8b6b-fdbd6e174d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternating D/G"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

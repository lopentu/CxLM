{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947f7331-799e-457e-9b05-f707abd298e9",
   "metadata": {},
   "source": [
    "## GAN preparation for MLM\n",
    "* Inputs:\n",
    "  * raw data: `../data/raw_cx_data.json` (10.01)\n",
    "  * CV splits: `../data/cv_splits_10.json` (10.01)\n",
    "* Outputs:\n",
    "  * (none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a002c61d-99e2-4e20-a1fd-8d739a3afd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b8acef-ef12-40d0-9d27-fde1bd8b7f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from hashlib import sha256\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizerFast, BertForMaskedLM, BertModel\n",
    "from import_conart import conart\n",
    "from conart.mlm_masks import batched_text_gan\n",
    "from conart import gan_utils as gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b571992-7a21-4000-80e1-bab0f9d8babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") \\\n",
    "         if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a62670-63f0-45f2-98b1-f8aa45b47866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../data/raw_cx_data.json\"\n",
    "with open(data_path, \"r\", encoding=\"UTF-8\") as fin:\n",
    "    data = json.load(fin)\n",
    "## Check data is the same\n",
    "h = sha256()\n",
    "h.update(pickle.dumps(data))\n",
    "data_hash = h.digest().hex()[:6]\n",
    "assert data_hash == \"4063b4\"\n",
    "len(data) # should be 11642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7556dcc-1a31-40c8-b142-9a41f7e4ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read cv splits\n",
    "with open(\"../data/cv_splits_10.json\", \"r\") as fin:\n",
    "    cv_splits = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46be44f6-5e00-4f5c-ac66-aec6f4441879",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce38a19-96c3-44c8-9985-a513758804be",
   "metadata": {},
   "source": [
    "## Checking input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d978cb6d-17eb-45b4-95a9-ece138f1f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, test_idxs = cv_splits[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1179ed-8332-46d9-99af-e5ca98da2f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'board': 'BabyMother',\n",
       " 'text': ['再', '擠', '也', '擠', '不', '出來', '了'],\n",
       " 'cnstr': ['O', 'BX', 'IX', 'IX', 'IX', 'IX', 'O'],\n",
       " 'slot': ['O', 'BV', 'BC', 'BV', 'BC', 'BV', 'O'],\n",
       " 'cnstr_form': ['v', '也', 'v', '不', 'X'],\n",
       " 'cnstr_example': ['擠', '也', '擠', '不', '出來']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = data[1211]\n",
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a826fa-19ed-488b-b4dd-bae771ec8094",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39107d4-75f8-46fe-9654-104ba544322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs_ds = TensorDataset(torch.tensor(train_idxs))\n",
    "test_idxs_ds = TensorDataset(torch.tensor(test_idxs))\n",
    "cx_lenc = LabelEncoder()\n",
    "cx_lenc.classes_ = [\"[PAD]\", \"BX\", \"IX\", \"O\"]\n",
    "slot_lenc = LabelEncoder()\n",
    "slot_lenc.classes_ = [\"[PAD]\", \"BC\", \"IC\", \"BV\", \"IV\", \"O\"]\n",
    "adv_lenc = LabelEncoder()\n",
    "adv_lenc.classes_ = [\"fake\", \"real\"]\n",
    "BV_id = slot_lenc.transform([\"BV\"])[0]\n",
    "IV_id = slot_lenc.transform([\"IV\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6a45f91-251c-4579-9f55-8d45a3efe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_collate_fn(X, data, cx_lenc, slot_lenc, device=\"cpu\"):  \n",
    "    idxs = [x[0].item() for x in X]\n",
    "    batch = batched_text_gan(data, idxs)\n",
    "    \n",
    "    real_tokens = tokenizer(batch[\"text\"], return_tensors=\"pt\", \n",
    "                          is_split_into_words=True, padding=True, truncation=True,\n",
    "                          return_token_type_ids=False, return_attention_mask=False)    \n",
    "    masked_tokens = tokenizer(batch[\"masked\"], return_tensors=\"pt\", \n",
    "                          is_split_into_words=True, padding=True, truncation=True)    \n",
    "    \n",
    "    cx_tags = [torch.tensor(cx_lenc.transform([\"[PAD]\"] + x + [\"[PAD]\"]))\n",
    "               for x in batch[\"cx_tags\"]]\n",
    "    cx_tags = pad_sequence(cx_tags, batch_first=True, padding_value=0)\n",
    "    cx_tags = cx_tags[:, :real_tokens.input_ids.size(1)]\n",
    "    \n",
    "    slot_tags = [torch.tensor(slot_lenc.transform([\"[PAD]\"] + x + [\"[PAD]\"])) \n",
    "                 for x in batch[\"slot_tags\"]]        \n",
    "    slot_tags = pad_sequence(slot_tags, batch_first=True, padding_value=0)\n",
    "    slot_tags = slot_tags[:, :real_tokens.input_ids.size(1)]\n",
    "    batch[\"cx_tags\"] = cx_tags.to(device)\n",
    "    batch[\"slot_tags\"] = slot_tags.to(device)\n",
    "    batch[\"real_text\"] = real_tokens.to(device)\n",
    "    batch[\"masked_text\"] = masked_tokens.to(device)\n",
    "    batch.update(gu.make_gendcr_labels(batch, adv_ids=[BV_id, IV_id]))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a4e27d8-5041-478a-a8b2-f7f650a70202",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = gan_collate_fn([test_idxs_ds[202], test_idxs_ds[203]], data, cx_lenc, slot_lenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "407f60b1-db74-4b14-940b-8821d1c1a4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'masked',\n",
       " 'cx_tags',\n",
       " 'slot_tags',\n",
       " 'real_text',\n",
       " 'masked_text',\n",
       " 'gen_labels',\n",
       " 'dcr_labels']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bb.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc6390e7-1e7e-4e85-8f7c-eee3f657cac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS],0', '也,5', '是,5', '直,5', '接,5', '退,3', '一,1', '退,3', '海,5', '闊,5', '天,5', '空,5', '了,5', '[SEP],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0']\n",
      "['[CLS],0', '也,3', '是,3', '直,3', '接,3', '退,1', '一,2', '退,2', '海,3', '闊,3', '天,3', '空,3', '了,3', '[SEP],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0', '[PAD],0']\n"
     ]
    }
   ],
   "source": [
    "## visual check\n",
    "print([f\"{a},{b.item()}\" for a,b in zip(tokenizer.convert_ids_to_tokens(bb[\"real_text\"].input_ids[0]), bb[\"slot_tags\"][0])])\n",
    "print([f\"{a},{b.item()}\" for a,b in zip(tokenizer.convert_ids_to_tokens(bb[\"real_text\"].input_ids[0]), bb[\"cx_tags\"][0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ce21a-ead2-4f60-82a3-6eaa1a7dc916",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Eye-balling fake/real labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3fcc4bb-1694-4561-87d8-bea26df61c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] 退 [PAD] 退 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## should only show variable sites\n",
    "tokenizer.decode(bb[\"real_text\"].input_ids.masked_fill(bb[\"gen_labels\"]!=1, 0)[0, :40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03121e98-ae1b-4cab-a864-ff60e105217e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] 也 是 直 接 [PAD] 一 [PAD] 海 闊 天 空 了 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## should only show non-fake sites (discriminator should say 'real')\n",
    "tokenizer.decode(bb[\"real_text\"].input_ids.masked_fill(bb[\"dcr_labels\"]!=1, 0)[0, :40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc2fcc0d-0ee4-4c41-8f0f-10662c6ac00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] 退 [PAD] 退 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## should only show fake sites (discriminator should say 'fake')\n",
    "tokenizer.decode(bb[\"real_text\"].input_ids.masked_fill(bb[\"dcr_labels\"]!=0, 0)[0, :40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f7ac3-3fbd-4462-8ee6-91dca4cd81fc",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fba799d8-1901-4b97-82bd-036d09e10c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel, BertForMaskedLM\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47553e9c-1166-4afc-b6ad-e367326add95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConartModelApricot(BertForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super(ConartModelApricot, self).__init__(config)\n",
    "        # inherit self.bert, self.cls (lm head) from super()\n",
    "        self.lm_cls = self.cls\n",
    "        self.tok_cls = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def G_params(self):\n",
    "        return [self.bert.parameters(), self.cls.parameters()]\n",
    "    \n",
    "    def D_params(self):\n",
    "        return [self.bert.parameters(), self.tok_cls.parameters()]\n",
    "    \n",
    "    def forward_G(self, X):\n",
    "        out = self.forward(X)\n",
    "        return self.lm_cls(out.last_hidden_state)\n",
    "    \n",
    "    def forward_D(self, X):\n",
    "        out = self.forward(X)\n",
    "        return self.tok_cls(out.last_hidden_state)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        tokens = X[\"masked_text\"]\n",
    "        cx_tags = X[\"cx_tags\"]\n",
    "        slot_tags = X[\"slot_tags\"]\n",
    "        bert_out = self.bert(**tokens, return_dict=True)\n",
    "        return bert_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8c3ba-54ab-48e2-a0ab-9374a21c1814",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31ec668b-2eda-4297-a768-a0ae856d5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = lambda x: gan_collate_fn(x, data, cx_lenc, slot_lenc, device)\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_idxs_ds, batch_size=batch_size, shuffle=True, \n",
    "                         collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_idxs_ds, batch_size=batch_size, shuffle=True, \n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20ecccfd-35fc-4a47-a231-7b59b8aa3c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  16\n",
      "Training dataset: 10477\n",
      "Testing dataset: 1165\n"
     ]
    }
   ],
   "source": [
    "print(\"batch size: \", batch_size)\n",
    "print(\"Training dataset:\", len(train_idxs_ds))\n",
    "print(\"Testing dataset:\", len(test_idxs_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "827d7764-c216-4924-b193-970d2af3538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ConartModelApricot were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['lm_cls.predictions.bias', 'lm_cls.predictions.transform.dense.bias', 'lm_cls.predictions.transform.LayerNorm.weight', 'lm_cls.predictions.transform.LayerNorm.bias', 'tok_cls.weight', 'lm_cls.predictions.decoder.weight', 'tok_cls.bias', 'lm_cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = BertForMaskedLM.from_pretrained('bert-base-chinese')\n",
    "model = ConartModelApricot.from_pretrained(\"ckiplab/bert-base-chinese\", num_labels=len(cx_lenc.classes_))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86d6e4ce-fa86-4c73-b575-8cbe7bc721db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 171, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = next(iter(test_loader))\n",
    "model.forward(bb).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e4985a8-22a5-464a-ad1f-54cae7d50710",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_probs = model.forward_G(bb)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a53be522-4874-4249-9086-0e3bbbb5ff26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([171, 21128])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14577bbb-f6b8-4aad-9951-56a044866442",
   "metadata": {},
   "source": [
    "## Generate Adversarial sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "535b9689-b5ea-4c7e-a41f-37b3c6f853ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]我們再\u001b[31m看(\u001b[4m翻\u001b[0;31m)\u001b[0m一\u001b[31m遍(\u001b[4m翻\u001b[0;31m)\u001b[0m繪本[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[CLS]愛\u001b[31m他(\u001b[4m吃\u001b[0;31m)\u001b[0m不\u001b[31m敢(\u001b[4m吃\u001b[0;31m)\u001b[0m很傷腦筋[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[CLS]\u001b[31m這(\u001b[4m算\u001b[0;31m)\u001b[0m一\u001b[31m舉(\u001b[4m算\u001b[0;31m)\u001b[0m應該比上次89萬投他票的人還多吧\n",
      "[CLS]密碼都\u001b[31m不(\u001b[4m改\u001b[0;31m)\u001b[0m一\u001b[31m樣(\u001b[4m改\u001b[0;31m)\u001b[0m吧[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[CLS]結果意識形態\u001b[31m是(\u001b[4m抹\u001b[0;31m)\u001b[0m一\u001b[31m不(\u001b[4m抹\u001b[0;31m)\u001b[0m[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[CLS]開始冒出一\u001b[31m次(\u001b[4m滴\u001b[0;31m)\u001b[0m又一\u001b[31m層(\u001b[4m滴\u001b[0;31m)\u001b[0m的冷汗[SEP][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[CLS]你怎麼不趕快去\u001b[31m;(\u001b[4m死\u001b[0;31m)\u001b[0m一\u001b[31m在(\u001b[4m死\u001b[0;31m)\u001b[0m[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[CLS]微軟也沒在管不過沒升win10其實有點\n",
      "[CLS]\u001b[31m大(\u001b[4m推\u001b[0;31m)\u001b[0m\u001b[31m查(\u001b[4m推\u001b[0;31m)\u001b[0m看這系列文章[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[CLS]剩下的都是開出來給高齡員工養老用的我以\n",
      "[CLS]最後他只差了15票就當選了因為說真的他\n",
      "[CLS]推樓上\u001b[31m、(\u001b[4m吵\u001b[0;31m)\u001b[0m歸\u001b[31m不(\u001b[4m吵\u001b[0;31m)\u001b[0m[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[CLS]\u001b[31m有(\u001b[4m想\u001b[0;31m)\u001b[0m一\u001b[31m面(\u001b[4m想\u001b[0;31m)\u001b[0m告訴自己沒怎麼多時間悲傷[SEP][PAD][PAD][PAD]\n",
      "[CLS]現在為了回家繞一堆路[UNK][UNK]上網\u001b[31m寫(\u001b[4m查\u001b[0;31m)\u001b[0m也\u001b[31m找(\u001b[4m查\u001b[0;31m)\u001b[0m不\u001b[31m見(\u001b[4m到\u001b[0;31m)\u001b[0m\n",
      "[CLS]就蓋子蓋著\u001b[31m，(\u001b[4m搖\u001b[0;31m)\u001b[0m一\u001b[31m看(\u001b[4m搖\u001b[0;31m)\u001b[0m就好了[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "[CLS]曜廷左\u001b[31m鄰(\u001b[4m看\u001b[0;31m)\u001b[0m右\u001b[31m右(\u001b[4m看\u001b[0;31m)\u001b[0m[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    }
   ],
   "source": [
    "lm_probs = model.forward_G(bb).softmax(dim=2)\n",
    "adv_out = gu.generate_adversarials(bb, lm_probs)\n",
    "gu.visualize_adv(adv_out, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360e047-8103-4a0f-a8dd-953dd28052e5",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a56298-3cbc-4a7a-a4b3-2230908cd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward_D(bb).argmax(2)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
